{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictive Analytics for Resource Allocation\n",
        "\n",
        "## Using Kaggle Breast Cancer Dataset\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Data preprocessing (clean, label, split)\n",
        "2. Training a Random Forest model to predict issue priority (high/medium/low)\n",
        "3. Evaluation using accuracy and F1-score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the breast cancer dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "df['target'] = cancer_data.target\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "# Display dataset info\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nBasic Statistics:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "### 3.1 Create Priority Labels (High/Medium/Low)\n",
        "\n",
        "For resource allocation, we'll map the diagnosis to priority levels:\n",
        "- **High Priority**: Malignant cases (target=0) - require immediate attention\n",
        "- **Low Priority**: Benign cases (target=1) - routine monitoring\n",
        "- **Medium Priority**: We'll create this based on feature combinations (e.g., borderline cases)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create priority labels based on diagnosis and feature combinations\n",
        "# High priority: Malignant (target=0)\n",
        "# Low priority: Benign (target=1) with low risk features\n",
        "# Medium priority: Benign cases with some concerning features\n",
        "\n",
        "def create_priority_labels(row):\n",
        "    \"\"\"\n",
        "    Create priority labels based on diagnosis and key features\n",
        "    \"\"\"\n",
        "    # High priority: Malignant cases\n",
        "    if row['target'] == 0:\n",
        "        return 'high'\n",
        "    \n",
        "    # For benign cases, check for concerning features\n",
        "    # Using mean radius and worst area as indicators\n",
        "    mean_radius = row['mean radius']\n",
        "    worst_area = row['worst area']\n",
        "    \n",
        "    # Medium priority: Benign but with elevated risk indicators\n",
        "    if mean_radius > df['mean radius'].quantile(0.7) or worst_area > df['worst area'].quantile(0.7):\n",
        "        return 'medium'\n",
        "    \n",
        "    # Low priority: Benign with low risk indicators\n",
        "    return 'low'\n",
        "\n",
        "# Apply the function to create priority labels\n",
        "df['priority'] = df.apply(create_priority_labels, axis=1)\n",
        "\n",
        "# Display priority distribution\n",
        "print(\"Priority Distribution:\")\n",
        "print(df['priority'].value_counts())\n",
        "print(\"\\nPriority Distribution (%):\")\n",
        "print(df['priority'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize priority distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['priority'].value_counts().plot(kind='bar', color=['red', 'orange', 'green'])\n",
        "plt.title('Priority Distribution')\n",
        "plt.xlabel('Priority Level')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Clean and Prepare Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df.drop(['target', 'priority'], axis=1)\n",
        "y = df['priority']\n",
        "\n",
        "# Encode priority labels to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(\"Feature shape:\", X.shape)\n",
        "print(\"Target shape:\", y_encoded.shape)\n",
        "print(\"\\nLabel mapping:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"{i}: {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame for better readability\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "print(\"Features standardized successfully\")\n",
        "print(\"\\nScaled features statistics:\")\n",
        "X_scaled_df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Split Data into Training and Testing Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_encoded, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining set priority distribution:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"  {label_encoder.inverse_transform([u])[0]}: {c} ({c/len(y_train)*100:.1f}%)\")\n",
        "print(f\"\\nTest set priority distribution:\")\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"  {label_encoder.inverse_transform([u])[0]}: {c} ({c/len(y_test)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Random Forest Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Model training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions generated successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Calculate F1-score (macro average for multi-class)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# Calculate F1-score per class\n",
        "train_f1_per_class = f1_score(y_train, y_train_pred, average=None)\n",
        "test_f1_per_class = f1_score(y_test, y_test_pred, average=None)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTraining Set:\")\n",
        "print(f\"  Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-Score (Macro): {train_f1:.4f}\")\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-Score (Macro): {test_f1:.4f}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"F1-Score per Class (Test Set):\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"  {label.capitalize()}: {test_f1_per_class[i]:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report (Test Set):\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(\n",
        "    y_test, y_test_pred, \n",
        "    target_names=label_encoder.classes_,\n",
        "    digits=4\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 10 Most Important Features:\")\n",
        "print(\"=\" * 60)\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Visualize top features\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary DataFrame\n",
        "performance_summary = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'F1-Score (Macro)', 'F1-Score (High)', 'F1-Score (Medium)', 'F1-Score (Low)'],\n",
        "    'Training Set': [\n",
        "        f\"{train_accuracy:.4f}\",\n",
        "        f\"{train_f1:.4f}\",\n",
        "        f\"{train_f1_per_class[0]:.4f}\",\n",
        "        f\"{train_f1_per_class[1]:.4f}\",\n",
        "        f\"{train_f1_per_class[2]:.4f}\"\n",
        "    ],\n",
        "    'Test Set': [\n",
        "        f\"{test_accuracy:.4f}\",\n",
        "        f\"{test_f1:.4f}\",\n",
        "        f\"{test_f1_per_class[0]:.4f}\",\n",
        "        f\"{test_f1_per_class[1]:.4f}\",\n",
        "        f\"{test_f1_per_class[2]:.4f}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL PERFORMANCE METRICS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(performance_summary.to_string(index=False))\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. ✅ **Data Preprocessing**: Successfully cleaned, labeled, and split the dataset\n",
        "2. ✅ **Model Training**: Trained a Random Forest classifier to predict priority levels\n",
        "3. ✅ **Model Evaluation**: Evaluated using accuracy and F1-score metrics\n",
        "\n",
        "The model can be used for resource allocation by prioritizing cases based on predicted priority levels.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
